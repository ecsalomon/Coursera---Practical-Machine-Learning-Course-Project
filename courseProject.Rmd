---
title: "Predicting Manner of Barbell Lifts from Accelerometers"
author: "Erika Salomon"
date: "August 16, 2015"
output: html_document
---

## Introduction 

Personal tracking devices have allowed people to record a lot of information about what they are doing---how many steps they take, how many flights of stairs they climb, etc. The next frontier in movement tracking may be identifying *how* something was done. This project uses machine learning on data from the personal activity monitors of six participants performing barbell lifts to predict whether the exercise was performed correctly or with one of four common errors (i.e., throwing the elbows to the front, lifting the dumbbell only halfway, lowering the dumbbell only halfway, and throwing the hips to the front).[^fn-data] Monitors were attched to participants' belts, forearms, arms, and dumbells and tracking data was recorded over 10 repetitions of each of the five performance types (correct plus  the four errors).


## Preparing for data analysis

This project makes use of several R packages (see code chunk below), which must be loaded prior to running the analyses. 

```{r}
library(caret)
library(lattice)
library(ggplot2)
library(randomForest)
library(BioSeqClass)
library(doParallel)
```

### Some performance tweaks

The `doParallel` class allows the use of multiple processor threads, which speeds up the training of machine learning algorithms. For this project, I have used four threads for training. The BioSeqClass package imports some data to the environment that is not used in the project, so I will also clear the environment before beginning to improve performance.

```{r}
registerDoParallel(cores=4)
rm(list = ls())
```


### Loading and cleaning data

The `getData.R` file in the repository contains a function to download and/or import the data set for this project. 

```{r, cache = TRUE}
source("getData.R")
data <- getData("training")
head(data)
```

The initial data set contains many columns, several of which have mostly missing values. It also contains several columns such as time stamps that, while they may be good indicators of performance type, will not be useful for making predictions outside of the sample. Before beginning my analysis, I will remove these columns from the data set.

```{r, cache = TRUE}
dataReduce <- data[, c(8:11, 37:49, 60:68, 84:86, 102, 113:124, 140, 151:160)]
```


## Machine learning approach

The goal of this project is to be able to predict the manner in which a barbell lift was done. I will use a random forest to make this prediction, as it is one of the best-performing machine learning classification algorithms. 

### Feature extraction and selection

For feature extraction and selection, I will compare four different approaches: 

1. using *all* of the features in the reduced data set
2. using principal components explaining 50% of the variance in the feature set
3. removing features that highly correlated with other features
4. selecting features on a combination of their intercorrelations with each other and their correlation with the outcome class

Each of these approaches to feature selection will be used to train a random forest algorithm using repeated (3 times) 10-fold cross-validation. 

```{r, cache = TRUE}
trainSettings <- trainControl(method="repeatedcv", number=10, repeats=3)
```

### Model selection

These approaches will produce four different models whose performance will be compared on several dimensions:

1. their interpretability
2. their speed
3. their out of sample error on a common validation set

The final sample will then be evaluated on a witheld test set to estimate its out of sample error rate. 

This approach requires that the data be divded into training, validation, and test sets. Half of the data will be used for training, one quarter for validation, and one quarter for testing the final model.

```{r, cache = TRUE}
set.seed(3)
trainIndex <- createDataPartition(dataReduce$classe, p = 0.50, list = FALSE)
training <- dataReduce[trainIndex,]
set.seed(3)
testing <- dataReduce[-trainIndex,]
testIndex <- createDataPartition(testing$classe, p = 0.50,
                                list = FALSE)
validation <- testing[-testIndex,]
testing <- testing[testIndex,]
```






```{r, cache = TRUE}
set.seed(3)
modFit52 <- train(classe ~ ., method = "rf", data = training,
                  trControl = trainSettings)
modFit52$finalModel
predictions52 <- predict(modFit52$finalModel, newdata = validation)
table(predictions52, validation$classe)
```

```{r, cache=TRUE}
pca <- preProcess(training[, 1:52], method = "pca", thresh = .5)
trainingPCA <- predict(pca, training[, 1:52])
trainingPCA$classe <- training$classe
set.seed(3)
modFitPCA <- train(classe ~ ., method = "rf", data = trainingPCA,
                   trControl = trainSettings)
modFitPCA$finalModel

validationPCA <- predict(pca, validation[, 1:52])
predictionsPCA <- predict(modFitPCA$finalModel, newdata = validationPCA)

table(predictionsPCA, validation$classe)
```

```{r}
removeCols <- findCorrelation(cor(training[, -53]), cutoff = .6,
                              verbose = FALSE)
set.seed(3)
modFitCor <- train(classe ~ ., trControl = trainSettings,
                   method = "rf", data = training[, -removeCols])
modFitCor$finalModel
predictionsCor <- predict(modFitCor$finalModel, newdata = validation)
table(predictionsCor, validation$classe)
```

```{r}
keepCols <- selectWeka(train = training, evaluator="CfsSubsetEval",
                       search = "BestFirst")

set.seed(3)
modFitCFS <- train(classe ~ ., trControl = trainSettings, 
                   method = "rf", data = training[, c(keepCols, 53)])
modFitCFS$finalModel

predictionsCFS <- predict(modFitCFS$finalModel, newdata = validation)
saveRDS(modFitCFS, file="modFitCFS.rds")

table(predictionsCFS, validation$classe)

```



What you should submit

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. 

1. Your submission should consist of a link to a Github repo with your R markdown and compiled HTML file describing your analysis. Please constrain the text of the writeup to < 2000 words and the number of figures to be less than 5. It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).
2. You should also apply your machine learning algorithm to the 20 test cases available in the test data above. Please submit your predictions in appropriate format to the programming assignment for automated grading. See the programming assignment for additional details.


Does the submission build a machine learning algorithm to predict activity quality from activity monitors?

Do the authors describe what they expect the out of sample error to be and estimate the error appropriately with cross-validation?

_________________________

## References

[^fn-data]: The data used in this project are [freely available](http://groupware.les.inf.puc-rio.br/har). See also: Velloso, E., Bulling, A., Gellersen, H., Ugulino, W., & Fuks, H. (2013). Qualitative activity recognition of weight lifting exercises. *Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13)*.